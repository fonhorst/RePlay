#!/usr/bin/env bash

set -ex

BASE_IMAGE_TAG="slama-replay-3.2.0"
REPO="node2.bdcl:5000"

if [[ -z "${KUBE_NAMESPACE}" ]]
then
  KUBE_NAMESPACE=spark-lama-exps
fi

if [[ -z "${IMAGE_TAG}" ]]
then
  IMAGE_TAG=${BASE_IMAGE_TAG}
fi


if [[ -z "${REPO}" ]]
then
  REPO=""
  echo "REPO var is not defined! Setting it to ${REPO}"
  IMAGE=spark-py-replay:${IMAGE_TAG}
  BASE_SPARK_IMAGE=spark-py:${BASE_IMAGE_TAG}
else
  IMAGE=${REPO}/spark-py-replay:${IMAGE_TAG}
  BASE_SPARK_IMAGE=${REPO}/spark-py:${BASE_IMAGE_TAG}
fi


function build_airflow_image() {
  cp ../LightAutoML/dist/SparkLightAutoML_DEV-0.3.2-py3-none-any.whl .

  cp ../LightAutoML/jars/spark-lightautoml_2.12-0.1.1.jar .

  cp experiments/docker/yarn-site.xml yarn-site.xml

  cp experiments/docker/core-site.xml core-site.xml

  poetry build

  poetry export --without-hashes -f requirements.txt > requirements.txt

  docker build -t node2.bdcl:5000/airflow-worker:latest -f experiments/docker/airflow-worker.dockerfile .

  rm core-site.xml
  rm yarn-site.xml
  rm SparkLightAutoML_DEV-0.3.2-py3-none-any.whl
  rm spark-lightautoml_2.12-0.1.1.jar
}

function push_airflow_image() {
  docker push node2.bdcl:5000/airflow-worker:latest
}

function build_spark_executor_image() {
  export SPARK_VERSION=3.2.0
  export HADOOP_VERSION=3.2

  if [ ! -d "/tmp/spark-build-dir/spark" ]
  then
    mkdir -p /tmp/spark-build-dir
    cd /tmp/spark-build-dir

    wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
      && tar -xvzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
      && mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark \
      && rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
  else
    cd /tmp/spark-build-dir
  fi
  # create images with names:
  # - ${REPO}/spark:${BASE_IMAGE_TAG}
  # - ${REPO}/spark-py:${BASE_IMAGE_TAG}
  # the last is equal to BASE_SPARK_IMAGE

  if [[ ! -z "${REPO}" ]]
  then
    ./spark/bin/docker-image-tool.sh -r ${REPO} -t ${BASE_IMAGE_TAG} \
      -p spark/kubernetes/dockerfiles/spark/bindings/python/Dockerfile \
      build

    ./spark/bin/docker-image-tool.sh -r ${REPO} -t ${BASE_IMAGE_TAG} push
  else
      ./spark/bin/docker-image-tool.sh -t ${BASE_IMAGE_TAG} \
      -p spark/kubernetes/dockerfiles/spark/bindings/python/Dockerfile \
      build
  fi
}

function build_spark_replay_image() {
  cp ../LightAutoML/dist/SparkLightAutoML_DEV-0.3.2-py3-none-any.whl .

  cp ../LightAutoML/jars/spark-lightautoml_2.12-0.1.1.jar .

  poetry build

  poetry export --without-hashes -f requirements.txt > requirements.txt

  docker build \
    --build-arg base_image=${BASE_SPARK_IMAGE} \
    -t ${IMAGE} \
    -f experiments/docker/spark-py-replay.dockerfile \
    .

  if [[ ! -z "${REPO}" ]]
  then
    docker push ${IMAGE}
  fi

  rm SparkLightAutoML_DEV-0.3.2-py3-none-any.whl
  rm spark-lightautoml_2.12-0.1.1.jar
}

function airflow_sync_dags() {
  rsync -v --perms --chmod=u+rwx,g+rwx,o+rwx experiments/dag_*.py  node2.bdcl:/mnt/ess_storage/DN_1/storage/Airflow/dags
}

function airflow_port_forward() {
    kubectl -n airflow port-forward svc/airflow-webserver 8080:8080
}

function help() {
  echo "
  List of commands.
    help - prints this message
  "
}

function main () {
    cmd="$1"

    if [ -z "${cmd}" ]
    then
      echo "No command is provided."
      help
      exit 1
    fi

    shift 1

    echo "Executing command: ${cmd}"

    case "${cmd}" in

    "help")
        help
        ;;

    "build-airflow-image")
        build_airflow_image
        ;;

    "push-airflow-image")
        push_airflow_image
        ;;

    "build-spark-executor-image")
        build_spark_executor_image
        ;;

    "build-spark-replay-image")
        build_spark_replay_image
        ;;

    "sync-dags")
        airflow_sync_dags
        ;;

    "airflow-port-forward")
        airflow_port_forward
        ;;

    *)
        echo "Unknown command: ${cmd}"
        ;;

    esac
}

main "${@}"
