#!/usr/bin/env bash

set -ex

BASE_IMAGE_TAG="slama-replay-3.2.0"
REPO="node2.bdcl:5000"

if [[ -z "${KUBE_NAMESPACE}" ]]
then
  KUBE_NAMESPACE=spark-lama-exps
fi

if [[ -z "${IMAGE_TAG}" ]]
then
  IMAGE_TAG=${BASE_IMAGE_TAG}
fi


if [[ -z "${REPO}" ]]
then
  REPO=""
  echo "REPO var is not defined! Setting it to ${REPO}"
  IMAGE=spark-py-replay:${IMAGE_TAG}
  BASE_SPARK_IMAGE=spark-py:${BASE_IMAGE_TAG}
else
  IMAGE=${REPO}/spark-py-replay:${IMAGE_TAG}
  BASE_SPARK_IMAGE=${REPO}/spark-py:${BASE_IMAGE_TAG}
fi


function build_airflow_image() {
  mkdir -p /tmp/replay_airflow_image_files

  cp ../LightAutoML/dist/SparkLightAutoML_DEV-0.3.2-py3-none-any.whl /tmp/replay_airflow_image_files/

  cp experiments/docker/yarn-site.xml /tmp/replay_airflow_image_files/yarn-site.xml

  cp experiments/docker/core-site.xml /tmp/replay_airflow_image_files/core-site.xml

  poetry build

  poetry export --without-hashes -f requirements.txt > /tmp/replay_airflow_image_files/requirements.txt

  docker build -t node2.bdcl:5000/airflow-worker:latest -f experiments/docker/airflow-worker.dockerfile /tmp/replay_airflow_image_files

  rm -r /tmp/replay_airflow_image_files
}

function push_airflow_image() {
  docker push node2.bdcl:5000/airflow-worker:latest
}

function build_spark_executor_image() {
  export SPARK_VERSION=3.2.0
  export HADOOP_VERSION=3.2

  if [ ! -d "/tmp/spark-build-dir/spark" ]
  then
    mkdir -p /tmp/spark-build-dir
    cd /tmp/spark-build-dir

    wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
      && tar -xvzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
      && mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark \
      && rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz
  else
    cd /tmp/spark-build-dir
  fi
  # create images with names:
  # - ${REPO}/spark:${BASE_IMAGE_TAG}
  # - ${REPO}/spark-py:${BASE_IMAGE_TAG}
  # the last is equal to BASE_SPARK_IMAGE

  if [[ ! -z "${REPO}" ]]
  then
    ./spark/bin/docker-image-tool.sh -r ${REPO} -t ${BASE_IMAGE_TAG} \
      -p spark/kubernetes/dockerfiles/spark/bindings/python/Dockerfile \
      build

    ./spark/bin/docker-image-tool.sh -r ${REPO} -t ${BASE_IMAGE_TAG} push
  else
      ./spark/bin/docker-image-tool.sh -t ${BASE_IMAGE_TAG} \
      -p spark/kubernetes/dockerfiles/spark/bindings/python/Dockerfile \
      build
  fi
}

function build_spark_replay_image() {
  cp ../LightAutoML/dist/SparkLightAutoML_DEV-0.3.2-py3-none-any.whl .

  cp ../LightAutoML/jars/spark-lightautoml_2.12-0.1.1.jar .

  poetry build

  poetry export --without-hashes -f requirements.txt > requirements.txt

  docker build \
    --build-arg base_image=${BASE_SPARK_IMAGE} \
    -t ${IMAGE} \
    -f experiments/docker/spark-py-replay.dockerfile \
    .

  if [[ ! -z "${REPO}" ]]
  then
    docker push ${IMAGE}
  fi

  rm SparkLightAutoML_DEV-0.3.2-py3-none-any.whl
  rm spark-lightautoml_2.12-0.1.1.jar
}

function airflow_sync_dags() {
  if [[ -z "${USERNAME_ON_CLUSTER}" ]]; then
      SUFFIX=""
  else
      SUFFIX="_${USERNAME_ON_CLUSTER}"
  fi

  rsync -v --perms --chmod=u+rwx,g+rwx,o+rwx experiments/dag_*"${SUFFIX}".py node2.bdcl:/mnt/ess_storage/DN_1/storage/Airflow/dags
}

function airflow_port_forward() {
    kubectl -n airflow port-forward svc/airflow-webserver 8080:8080
}

function upd_wheels_and_jars() {
  if [[ -z "${USERNAME_ON_CLUSTER}" ]]; then
      SUFFIX=""
  else
      SUFFIX="_${USERNAME_ON_CLUSTER}"
  fi

  echo "building RePlay wheel"
  poetry build

  SPARK_SUBMIT_FILES_DIR="/mnt/ess_storage/DN_1/storage/SLAMA/kaggle_used_cars_dataset/spark_submit_files/"
  # copying RePlay wheel and jar files to .../spark_submit_files/ mounted in airflow container
  scp dist/replay_rec-0.10.0-py3-none-any.whl node2.bdcl:"${SPARK_SUBMIT_FILES_DIR}"replay_rec-0.10.0-py3-none-any"${SUFFIX}".whl
  scp scala/target/scala-2.12/replay_2.12-0.1.jar node2.bdcl:"${SPARK_SUBMIT_FILES_DIR}"replay_2.12-0.1"${SUFFIX}".jar
}

function upd_replay_package() {
  if [[ -z "${USERNAME_ON_CLUSTER}" ]]; then
      echo "USERNAME_ON_CLUSTER environment variable is required!"
      exit 1
  fi

  rsync -avr --perms --chmod=u+rwx,g+rwx,o+rwx replay node2.bdcl:/mnt/ess_storage/DN_1/storage/Airflow/dags/"${USERNAME_ON_CLUSTER}"_packages/
}

function generate_suffixed_files() {
  tmp_folder="./tmp_generated_suffixed_files"

  rm -rf ${tmp_folder}

  mkdir ${tmp_folder}

  ls -l dag_*.py | awk -F'[. ]' '{print $10}' \
    | xargs -L 1 /bin/bash -c \
    'cat $0.py |  sed -e "s/dag_entities/dag_entities_nbutakov/g" -e "s/dag_utils/dag_utils_nbutakov/g" > '${tmp_folder}'/$0_nbutakov.py'
}

function help() {
  echo "
  List of commands.
    help - prints this message
  "
}

function main () {
    cmd="$1"

    if [ -z "${cmd}" ]
    then
      echo "No command is provided."
      help
      exit 1
    fi

    shift 1

    echo "Executing command: ${cmd}"

    case "${cmd}" in

    "help")
        help
        ;;

    "build-airflow-image")
        build_airflow_image
        ;;

    "push-airflow-image")
        push_airflow_image
        ;;

    # obsolete
    "build-spark-executor-image")
        build_spark_executor_image
        ;;

    # obsolete
    "build-spark-replay-image")
        build_spark_replay_image
        ;;

    "sync-dags")
        airflow_sync_dags
        ;;

    "airflow-port-forward")
        airflow_port_forward
        ;;

    "upd-wheels-and-jars")
        upd_wheels_and_jars
        ;;

    "upd-replay-pkg")
        upd_replay_package
        ;;

    "suffix-files")
        generate_suffixed_files
        ;;

    *)
        echo "Unknown command: ${cmd}"
        ;;

    esac
}

main "${@}"
